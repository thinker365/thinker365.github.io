(window.webpackJsonp=window.webpackJsonp||[]).push([[59],{577:function(s,a,t){"use strict";t.r(a);var n=t(35),e=Object(n.a)({},(function(){var s=this,a=s.$createElement,t=s._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("p"),t("div",{staticClass:"table-of-contents"},[t("ul",[t("li",[t("a",{attrs:{href:"#所需数学知识"}},[s._v("所需数学知识")])]),t("li",[t("a",{attrs:{href:"#知识储备"}},[s._v("知识储备")])]),t("li",[t("a",{attrs:{href:"#获取数据"}},[s._v("获取数据")])]),t("li",[t("a",{attrs:{href:"#数据处理"}},[s._v("数据处理")])]),t("li",[t("a",{attrs:{href:"#特征工程"}},[s._v("特征工程")])]),t("li",[t("a",{attrs:{href:"#机器学习算法"}},[s._v("机器学习算法")]),t("ul",[t("li",[t("a",{attrs:{href:"#有监督学习"}},[s._v("有监督学习")])]),t("li",[t("a",{attrs:{href:"#无监督学习"}},[s._v("无监督学习")])]),t("li",[t("a",{attrs:{href:"#强化学习"}},[s._v("强化学习")])]),t("li",[t("a",{attrs:{href:"#"}})])])]),t("li",[t("a",{attrs:{href:"#参考"}},[s._v("参考")])])])]),t("p"),s._v(" "),t("h2",{attrs:{id:"所需数学知识"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#所需数学知识"}},[s._v("#")]),s._v(" 所需数学知识")]),s._v(" "),t("ol",[t("li",[s._v("微积分")]),s._v(" "),t("li",[s._v("线性代数")]),s._v(" "),t("li",[s._v("概率论")]),s._v(" "),t("li",[s._v("最优化方法")])]),s._v(" "),t("h2",{attrs:{id:"知识储备"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#知识储备"}},[s._v("#")]),s._v(" 知识储备")]),s._v(" "),t("p",[s._v("scikit-learn")]),s._v(" "),t("h2",{attrs:{id:"获取数据"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#获取数据"}},[s._v("#")]),s._v(" 获取数据")]),s._v(" "),t("ol",[t("li",[s._v("sklearn数据集\n"),t("ul",[t("li",[s._v("获取数据集\n"),t("ul",[t("li",[s._v("小数据集，sklearn.datasets.load_*()")]),s._v(" "),t("li",[s._v("大数据集，sklearn.datasets.fetch_*()")])])]),s._v(" "),t("li",[s._v("数据集包含的内容\n"),t("ul",[t("li",[s._v("data：特征数据数组")]),s._v(" "),t("li",[s._v("target：目标值数组")]),s._v(" "),t("li",[s._v("DESCR：数据描述")]),s._v(" "),t("li",[s._v("feature_name：特征名")]),s._v(" "),t("li",[s._v("target_names：目标值名")])])])])]),s._v(" "),t("li",[s._v("查看数据分布\n"),t("ul",[t("li",[s._v("SEABORN.LMPLOT()\n"),t("ul",[t("li",[s._v("x,y表示横纵坐标内容")]),s._v(" "),t("li",[s._v("data表示需要关联到的数据集")]),s._v(" "),t("li",[s._v("hue表示最后显示的种类（目标值）")]),s._v(" "),t("li",[s._v("fit_reg表示是否进行线性拟合")])])])])]),s._v(" "),t("li",[s._v("数据集分类\n"),t("ul",[t("li",[s._v("分类\n"),t("ul",[t("li",[s._v("测试集，模型评估")]),s._v(" "),t("li",[s._v("训练集，模型构建")])])]),s._v(" "),t("li",[s._v("API\n"),t("ul",[t("li",[s._v("sklearn.model_selection.train_test_split()")]),s._v(" "),t("li",[s._v("返回值\n"),t("ul",[t("li",[s._v("训练集的特征值")]),s._v(" "),t("li",[s._v("测试集的特征值")]),s._v(" "),t("li",[s._v("训练集的目标值")]),s._v(" "),t("li",[s._v("测试集的目标值")])])]),s._v(" "),t("li",[s._v("入参\n"),t("ul",[t("li",[s._v("x表示数据集的特征值")]),s._v(" "),t("li",[s._v("y表示数据集的目标值")]),s._v(" "),t("li",[s._v("test_size表示测试集的大小，比如训练集70~80%，测试集20~30%")]),s._v(" "),t("li",[s._v("random_stat表示随机种子，不同的种子会造成不同的随机采样结果，相同的种子采样结果相同")])])])])])])])]),s._v(" "),t("h2",{attrs:{id:"数据处理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据处理"}},[s._v("#")]),s._v(" 数据处理")]),s._v(" "),t("ol",[t("li",[s._v("缺失值处理")]),s._v(" "),t("li",[s._v("缩小数据范围")]),s._v(" "),t("li",[s._v("异常数据处理")]),s._v(" "),t("li",[s._v("。。。")])]),s._v(" "),t("h2",{attrs:{id:"特征工程"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#特征工程"}},[s._v("#")]),s._v(" 特征工程")]),s._v(" "),t("ol",[t("li",[s._v("定义：把数据转为机器更容易识别处理的内容")]),s._v(" "),t("li",[s._v("内容\n"),t("ul",[t("li",[s._v("特征预处理\n"),t("ul",[t("li",[s._v("定义：通过转换函数将特征数据转换为更加适合算法模型的特征数据过程")]),s._v(" "),t("li",[s._v("内容\n"),t("ul",[t("li",[s._v("归一化\n"),t("ul",[t("li",[s._v("通过对原始数据进行变换，把数据映射到[0，1]之间，0，1是默认区间")]),s._v(" "),t("li",[s._v("API：sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)...)")]),s._v(" "),t("li",[s._v("注意最大值最小值是变化的，非常容易受异常值影响，鲁棒性较差，适合传统精确小数据")])])]),s._v(" "),t("li",[s._v("标准化\n"),t("ul",[t("li",[s._v("通过对原始数据进行变换，把数据变换到均值为0，标准差为1的范围")]),s._v(" "),t("li",[s._v("API：sklearn.preprocessing.StandardScaler()")]),s._v(" "),t("li",[s._v("在已有样本足够的情况下比较稳定，适合现在嘈杂大数据场景")])])])])])])]),s._v(" "),t("li",[s._v("特征抽取\n"),t("ul",[t("li",[s._v("定义：将任意数据转为可用于机器学习的数字特征")]),s._v(" "),t("li",[s._v("分类\n"),t("ul",[t("li",[s._v("字典特征\n"),t("ul",[t("li",[s._v("sklearn.feature_extraction.DictVectorizer()")]),s._v(" "),t("li",[s._v("对于特征中存在类别信息的做one-hot编码处理")])])]),s._v(" "),t("li",[s._v("文本特征\n"),t("ul",[t("li",[s._v("sklearn.feature_extraction.text.CountVectorizer()，抽取英文")]),s._v(" "),t("li",[s._v("先使用jieba.cut()，然后使用sklearn.feature_extraction.text.CountVectorizer()，抽取中文")]),s._v(" "),t("li",[s._v("tf-idf，用于评估一个字词在一个文件集或一个语料库中的其中一份文件的重要性，from sklearn.feature_extraction import TfidfVectorizer")])])]),s._v(" "),t("li",[s._v("图像特征")])])])])]),s._v(" "),t("li",[s._v("特征降维\n"),t("ul",[t("li",[s._v("定义：在某些限定条件下，降低随机变量个数")]),s._v(" "),t("li",[s._v("方式\n"),t("ul",[t("li",[s._v("特征选择\n"),t("ul",[t("li",[s._v("过滤式：消除低方差的一些特征"),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("feature_selection "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" VarianceThreshold\nVarianceThreshold"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("threshold"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])])]),s._v(" "),t("li",[s._v("相关系数\n"),t("ul",[t("li",[s._v("皮尔逊相关系数")]),s._v(" "),t("li",[s._v("判断标准：|r|<0.4为低度相关，0.4≤|r|<0.7为显著相关，0.7≤|r|<1为高度线性相关")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scipy"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("stats "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" pearsonr\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])])])])]),s._v(" "),t("li",[s._v("主成分分析\n"),t("ul",[t("li",[s._v("高维数组转低维数组")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("decomposition "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" PCA\nPCA"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("n_components"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])])])])])])])])])]),s._v(" "),t("h2",{attrs:{id:"机器学习算法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#机器学习算法"}},[s._v("#")]),s._v(" 机器学习算法")]),s._v(" "),t("h3",{attrs:{id:"有监督学习"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#有监督学习"}},[s._v("#")]),s._v(" 有监督学习")]),s._v(" "),t("ol",[t("li",[s._v("决策树")]),s._v(" "),t("li",[s._v("线性模型")]),s._v(" "),t("li",[s._v("KNN")]),s._v(" "),t("li",[s._v("Bayes")]),s._v(" "),t("li",[s._v("LDA")])]),s._v(" "),t("h3",{attrs:{id:"无监督学习"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#无监督学习"}},[s._v("#")]),s._v(" 无监督学习")]),s._v(" "),t("ol",[t("li",[s._v("降维")]),s._v(" "),t("li",[s._v("聚类")])]),s._v(" "),t("h3",{attrs:{id:"强化学习"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#强化学习"}},[s._v("#")]),s._v(" 强化学习")]),s._v(" "),t("ol",[t("li",[s._v("策略迭代")]),s._v(" "),t("li",[s._v("价值迭代")]),s._v(" "),t("li",[s._v("蒙特卡罗算法")]),s._v(" "),t("li",[s._v("时序差分算法")])]),s._v(" "),t("h3",{attrs:{id:""}},[t("a",{staticClass:"header-anchor",attrs:{href:"#"}},[s._v("#")])]),s._v(" "),t("ol",{attrs:{start:"5"}},[t("li",[s._v("K近邻算法\n"),t("ul",[t("li",[s._v("理解：根据最近距离，判断属于哪个类别")]),s._v(" "),t("li",[s._v("API\n"),t("ul",[t("li",[s._v("algorithm:{'auto','ball_tree','kd_tree','brute'}")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("neighbors "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" KNeighborsClassifier\nKNeighborsClassifier"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("n_neighbors"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("algorithm"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'auto'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])])]),s._v(" "),t("li",[s._v("距离度量\n"),t("ul",[t("li",[s._v("欧式距离")]),s._v(" "),t("li",[s._v("曼哈顿距离")]),s._v(" "),t("li",[s._v("切比雪夫距离")]),s._v(" "),t("li",[s._v("闵可夫斯基距离，当p=1就是曼哈顿距离，当p=1就是欧式距离，当p=∞就是切比雪夫距离，")]),s._v(" "),t("li",[s._v("马氏距离")]),s._v(" "),t("li",[s._v("标准化欧式距离")]),s._v(" "),t("li",[s._v("汉明距离")]),s._v(" "),t("li",[s._v("杰卡德距离")]),s._v(" "),t("li",[s._v("余弦距离")])])]),s._v(" "),t("li",[s._v("K值选择\n"),t("ul",[t("li",[s._v("K值过小，容易受异常点影响，意味着整体模型变得复杂，容易发生过拟合")]),s._v(" "),t("li",[s._v("K值过大，受到样本均衡问题，K值增大意味着整体模型变得简单，容易发生欠拟合")]),s._v(" "),t("li",[s._v("K=N，N表示训练样本个数，发生完全不足取的情况，此时训练的模型完全失效，判断的类别只会选择样本中样本数最多的类别")])])]),s._v(" "),t("li",[s._v("KD树\n"),t("ul",[t("li",[s._v("对训练数据进行快速K近邻搜索")]),s._v(" "),t("li",[s._v("实现思路\n"),t("ul",[t("li",[s._v("构建树：构造根节点；通过递归方式不断的构造；直到子区域没有节点，停止")]),s._v(" "),t("li",[s._v("最近领域搜索：构造一个队列，使遍历过的点在这个队列里；是否过了超平面的判断，如果没有过，跳过，过了的话，需要把当前的子节点添加进去；循环操作，求得最近的点")])])])])]),s._v(" "),t("li",[s._v("K近邻总结\n"),t("ul",[t("li",[s._v("优点：简单，易于理解，易于实现")]),s._v(" "),t("li",[s._v("缺点：懒惰算法，对测试样本分类时计算量大，内存开销大；必须指定K值，指定不当，则分类精度不能保证")])])]),s._v(" "),t("li",[s._v("使用场景：小数据场景，几千到几万样本")])])]),s._v(" "),t("li",[s._v("线性回归\n"),t("ul",[t("li",[s._v("定义：利用回归方程，对一个或多个自变量（特征值）和因变量（目标值）之前的关系，进行建模的一种分析方式")]),s._v(" "),t("li",[s._v("分类：线性关系和非线性关系")]),s._v(" "),t("li",[s._v("线性回归的损失和优化原理\n"),t("ul",[t("li",[s._v("损失函数，最小二乘")]),s._v(" "),t("li",[s._v("优化算法\n"),t("ul",[t("li",[s._v("正规方程：通过矩阵变换，可以一步进行求解，只适合小样本计算")]),s._v(" "),t("li",[s._v("梯度下降法\n"),t("ul",[t("li",[s._v("梯度：单变量（函数微分）和多变量（向量）")]),s._v(" "),t("li",[s._v("梯度下降思路：首先指定初始值点（完全随机）；知道一个学习率；循环迭代，直到最后判断周围点都比我大，我默认找到了最小点")]),s._v(" "),t("li",[s._v("α（学习率、步长）：不可以过大或过小，过大避开了极小点，过小步子太小，找到极小点，耗时长")]),s._v(" "),t("li",[s._v("“-”（为什么是负梯度）：梯度前面加负号，目的是找到最快梯度下降方向，梯度本身值是最快上升方向")])])])])])])]),s._v(" "),t("li",[s._v("API\n"),t("ul",[t("li",[s._v("正规方程对应的API")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("linear_model "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" LinearRegression\nLinearRegression"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("fit_intercept"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nLinearRegression"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("coef_  "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 回归系数")]),s._v("\nLinearRegression"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("intercept_  "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 偏置")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("ul",[t("li",[s._v("梯度下降法对应的API")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("linear_model "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" SGDRegressor\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("ul",[t("li",[s._v("评估－均方误差API")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("metrics "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" mean_squared_error\nmean_squared_error"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_true"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_pred"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])])])])]),s._v(" "),t("li",[s._v("线性回归改进之岭回归\n"),t("ul",[t("li",[s._v("理解：带有Ｌ２正则化的线性回归")]),s._v(" "),t("li",[s._v("API")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("linear_model "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Ridge\nRidge"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("alpha"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("normalize"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"deprecated"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("solver"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"auto"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nalpha表示正则化力度\nsolver优先选择随机平均梯度下降法SAG\nnormalize是否对特征值进行标准化\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br")])]),t("ul",[t("li",[s._v("岭回归中正则化变化对结果的影响：正则化力度越大，权重系数越小；反之。")]),s._v(" "),t("li",[s._v("回归算法总结\n"),t("ul",[t("li",[s._v("小规模数据：LinearRegression（不能解决拟合问题）；岭回归")]),s._v(" "),t("li",[s._v("大规模数据：SGDRegressor")])])])])]),s._v(" "),t("li",[s._v("逻辑回归（分类）\n"),t("ul",[t("li",[s._v("理解：解决二分类问题的利器；逻辑回归的输入就是线性回归的输出结果；激活函数：sigmoid函数")]),s._v(" "),t("li",[s._v("损失与优化：对数似然损失；提升原本属于1类的概率，降低原本是0类的概率")]),s._v(" "),t("li",[s._v("API")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("linear_model "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" LogisticRegression\nLogisticRegression"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("solver"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("penalty"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("C"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nsolver可选择的梯度下降方式\npenalty正则化方式\nC正则化力度\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br")])])]),s._v(" "),t("li",[s._v("决策树\n"),t("ul",[t("li",[s._v("理解：多个判断节点组成的树")]),s._v(" "),t("li",[s._v("分类原则\n"),t("ul",[t("li",[s._v("熵：就是混乱程度的度量，信息越混乱熵越高，反之；－plogp")]),s._v(" "),t("li",[s._v("信息增益：以某特征划分数据集前后熵的差值")]),s._v(" "),t("li",[s._v("信息增益比：优先选择类别比较多的特征作为节点")]),s._v(" "),t("li",[s._v("基尼值和基尼指数：从数据集中随机抽取样本，其类别标记不一致的概率，值越小，数据集的纯度越高")])])]),s._v(" "),t("li",[s._v("CART剪枝\n"),t("ul",[t("li",[s._v("目的：防止过度拟合")]),s._v(" "),t("li",[s._v("方法\n"),t("ul",[t("li",[s._v("预剪枝：通过叶子节点的样本数量判断；通过数的深度判断；通过熵值判断")]),s._v(" "),t("li",[s._v("后剪枝：把决策树生成后，再进行剪枝")])])])])]),s._v(" "),t("li",[s._v("API")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("tree "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" DecisionTreeClassifier\nDecisionTreeClassifier"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("criterion"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("max_depth"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ncriterion划分决策树的标准，默认gini，也可以使用entropy\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("ul",[t("li",[s._v("决策树总结\n"),t("ul",[t("li",[s._v("优点：可视化")]),s._v(" "),t("li",[s._v("缺点：创建不能很好推广数据的过于复杂的树，称为过拟合")]),s._v(" "),t("li",[s._v("改进：减枝cart算法，设置深度，设置叶子节点最小个数；随机森林")])])])])]),s._v(" "),t("li",[s._v("集成学习\n"),t("ul",[t("li",[s._v("简介：通过建立几个模型来决定单一预测问题，生成多个模型，各自独立学习，然后进行结合")]),s._v(" "),t("li",[s._v("BAGGING（互相牵制变壮）\n"),t("ul",[t("li",[s._v("流程：对数据进行重新采样；对数据集分别进行学习；平权投票，集成，产生结果")]),s._v(" "),t("li",[s._v("随机森林\n"),t("ul",[t("li",[s._v("bagging+决策树")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ensemble "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" RandomForestClassifier\nRandomForestClassifier"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("n_estimators"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("criterion"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("max_depth"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("max_features"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("bootstrap"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("min_samples_leaf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("min_samples_split"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nn_estimators森林里树木数量\ncriterion分割特征的测量方法\nmax_depth树的最大深度\nmax_features每个决策树的最大特征数量\nbootstrap是否有放回抽取\nmin_samples_split节点划分最小样本数\nmin_samples_leaf叶子节点的最小样本数\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br")])])]),s._v(" "),t("li",[s._v("优势：简单、方便、通用；可在原有算法上提高2%左右的泛化正确性")])])]),s._v(" "),t("li",[s._v("BOOSTING（弱弱组合变强）\n"),t("ul",[t("li",[s._v("流程\n"),t("ul",[t("li",[s._v("初始化训练数据，其权重相等")]),s._v(" "),t("li",[s._v("计算学习器在训练数据中的错误率")]),s._v(" "),t("li",[s._v("计算学习器的投票权重")]),s._v(" "),t("li",[s._v("根据权重对训练数据重新赋权")]),s._v(" "),t("li",[s._v("重复执行以上4步m次")]),s._v(" "),t("li",[s._v("对m个学习期进行加权投票")])])]),s._v(" "),t("li",[s._v("BAGGING和BOOSTING区别\n"),t("ul",[t("li",[s._v("数据方面：BAGGING对数据需要重新采样，BOOSTING调整数据")]),s._v(" "),t("li",[s._v("投票方面：BAGGING平权，BOOSTING加权")]),s._v(" "),t("li",[s._v("学习顺序：BAGGING并行，BOOSTING串行")]),s._v(" "),t("li",[s._v("主要作用：BAGGING过拟合，BOOSTING欠拟合")])])]),s._v(" "),t("li",[s._v("GBTD（梯度提升决策树）\n"),t("ul",[t("li",[s._v("算法由多颗决策树组成，所有树的结论累加起来做最终答案")]),s._v(" "),t("li",[s._v("公式：梯度下降+BOOSTING+决策树")]),s._v(" "),t("li",[s._v("主要执行思想：使用梯度下降法优化代价函数；使用一层决策树作为弱学习器，负梯度作为目标值；利用BOOSTING思想进行集成")])])])])])])]),s._v(" "),t("li",[s._v("K-MEANS\n"),t("ul",[t("li",[s._v("简介：无监督学习算法；将相似样本自动归类；组内距离最小化，组间距离最大化")]),s._v(" "),t("li",[s._v("API")])]),s._v(" "),t("div",{staticClass:"language-python line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cluster "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" KMeans\nKMeans"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("n_clusters"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nn_clusters开始的聚类中心数据\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("ul",[t("li",[s._v("实现流程\n"),t("ul",[t("li",[s._v("确定常数K")]),s._v(" "),t("li",[s._v("选定初始点为质心")]),s._v(" "),t("li",[s._v("重新计算每个类的质心")]),s._v(" "),t("li",[s._v("重复上述过程，直到质心不再改变")])])]),s._v(" "),t("li",[s._v("误差评估\n"),t("ul",[t("li",[s._v("肘部法：下降率突然变缓时，即认为是最佳的K值")]),s._v(" "),t("li",[s._v("SC系数：取值为[-1,1]，值越大越好")]),s._v(" "),t("li",[s._v("CH系数：分数s高则聚类效果好")])])]),s._v(" "),t("li",[s._v("算法优化\n"),t("ul",[t("li",[s._v("Canopy+kmeans， Canopy粗聚类配合kmeans")]),s._v(" "),t("li",[s._v("kmeans++，距离越远越容易成为新的质心")]),s._v(" "),t("li",[s._v("二分K-means，拆除SSE最大的簇")]),s._v(" "),t("li",[s._v("ISODATA，动态聚类")]),s._v(" "),t("li",[s._v("kerner kmeans，映射到高维空间")]),s._v(" "),t("li",[s._v("Mini-batch K-means，大数据集，分批聚类")])])])])])]),s._v(" "),t("h2",{attrs:{id:"参考"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[s._v("#")]),s._v(" 参考")]),s._v(" "),t("ol",[t("li",[t("a",{attrs:{href:"https://github.com/MLEveryday",target:"_blank",rel:"noopener noreferrer"}},[s._v("https://github.com/MLEveryday"),t("OutboundLink")],1)]),s._v(" "),t("li",[t("a",{attrs:{href:"http://seaborn.pydata.org/index.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("http://seaborn.pydata.org/index.html"),t("OutboundLink")],1)]),s._v(" "),t("li",[t("a",{attrs:{href:"https://www.sklearncn.cn/",target:"_blank",rel:"noopener noreferrer"}},[s._v("https://www.sklearncn.cn/"),t("OutboundLink")],1)]),s._v(" "),t("li",[t("a",{attrs:{href:"https://tianchi.aliyun.com/course?spm=5176.12281949.J_3941670930.8.493e2448d4Fjzu",target:"_blank",rel:"noopener noreferrer"}},[s._v("阿里云天池大数据"),t("OutboundLink")],1)]),s._v(" "),t("li",[t("a",{attrs:{href:"https://scikit-learn.org.cn/lists/1.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("社区网站"),t("OutboundLink")],1)]),s._v(" "),t("li",[t("a",{attrs:{href:"https://www.sklearncn.cn/",target:"_blank",rel:"noopener noreferrer"}},[s._v("ReadTheDoc"),t("OutboundLink")],1)])])])}),[],!1,null,null,null);a.default=e.exports}}]);